ES
倒排索引是什么？
ElasticSearch 存储的基本单元是索引，实现基于 Lucene，使用倒排索引的结构。
倒排索引的概念是相对于像MySQL这样的正向索引而言的。
● 正向索引是最传统的，根据文档id索引的方式来查找文档；
● 倒排索引则相反，是通过用户搜索的词条来匹配文档id，根据id获取文档；
向 ES 插入文档时，会对文档本身进行分词处理得到一系列词条，然后建立词条和文档的之间的倒排索引。当用户使用关键词搜索时，同样会进行分词处理，并通过倒排索引查询到与词条相关的文档。
倒排索引和 B+ 树索引的区别？
严格地说，这两类索引是不能在一起比较的，B+ 树描述的是索引的数据结构，而倒排索引是通过索引的组织形式来命名的。比如我们上面的例子中，倒排指的是关键词和文档列表的结构关系。
对于数据库来说，索引的作用是提高数据查询的性能，考虑到磁盘寻址的特性，选择了 B+ 树作为索引的实现结构，可以更好地实现通过主键以及通过区间范围查找的要求。
对于倒排索引，则是对应具体的应用场景，我们在搜索中是通过一些关键词，定位到具体的文档。所以倒排索引实现的是根据关键词，也就是分词的结果，去查找文档，或者不同的网页。
为什么B+树不适合做全文检索？
1. 全文检索的场景通常内容比较长，如果使用B+树，其结构可能会很深，IO效率低
2. 并且索引会失效，无法保证查询性能
3. 查询的精准度（相关度）比较差，无法和其他字段进行联动。
   定义副本、创建副本的好处是什么？
   副本是 分片的对应副本，用在极端负载条件下提高查询吞吐量或实现高可用性。
   所谓高可用主要指：如果某主分片1出了问题，对应的副本分片1会提升为主分片，保证集群的高可用。
   https://cloud.tencent.com/developer/article/1077592
   ES 分片了解吗？
   ES可以把一个完整的索引分成多个分片，这样的好处是可以把一个索引拆分成多个，分布到不同的节点上，从而构成分布式搜索。ES的分片分为主分片和副本分片：
   ● 主分片（Primary Shard），解决数据水平扩展的问题，通过主分片，将数据分布到集群内的所有节点上面。主分片数在索引创建的时候指定，之后不可以更改，除非重新索引。
   ● 副本分片（Replica Shard），解决数据的高可用问题，是主分片的拷贝。
   当集群扩容或缩小，Elasticsearch 将会自动在节点间迁移分片，以使集群保持平衡。
   为什么搜索是近实时的？
   因为文档索引在从内存缓存被写入到文件缓存系统时，虽然还没有进行提交未被 flush 到磁盘，但是缓冲区的内容已经被写入一个段（segment）中且新段可被搜索。这就是为什么我们说 Elasticsearch 是近实时搜索: 文档的变化并不是立即对搜索可见，但可以在 一秒之内变为可见。
   原文链接：https://blog.csdn.net/u010454030/article/details/79586072
   一个index是由若干个segment组成，随着每个segment的不断增长，我们索引一条数据后可能要经过分钟级别的延迟才能被搜索，为什么有种这么大的延迟，这里面的瓶颈点主要在磁盘。
   持久化一个segment需要fsync操作用来确保segment能够物理的被写入磁盘以真正的避免数据丢失，但是fsync操作比较耗时，所以它不能在每索引一条数据后就执行一次，如果那样索引和搜索的延迟都会非常之大。
   所以这里需要一个更轻量级的处理方式，从而保证搜索的延迟更小。这就需要用到上面提到的FileSystem Cache，所以在es中新增的document会被收集到indexing buffer区后被重写成一个segment然后直接写入filesystem cache中，这个操作是非常轻量级的，相对耗时较少，之后经过一定的间隔或外部触发后才会被flush到磁盘上，这个操作非常耗时。但只要sengment文件被写入cache后，这个sengment就可以打开和查询，从而确保在短时间内就可以搜到，而不用执行一个full commit也就是fsync操作，这是一个非常轻量级的处理方式而且是可以高频次的被执行，而不会破坏es的性能。
   在elasticsearch里面，这个轻量级的写入和打开一个cache中的segment的操作叫做refresh，默认情况下，es集群中的每个shard会每隔1秒自动refresh一次，这就是我们为什么说es是近实时的搜索引擎而不是实时的，也就是说给索引插入一条数据后，我们需要等待1秒才能被搜到这条数据，这是es对写入和查询一个平衡的设置方式，这样设置既提升了es的索引写入效率同时也使得es能够近实时检索数据。
   在并发情况下，Elasticsearch 如果保证读写一致？
   ● 可以通过版本号使用乐观并发控制，以确保新版本不会被旧版本覆盖，由应用层来处理具体的冲突；
   ● 另外对于写操作，一致性级别支持 quorum/one/all，默认为 quorum，即只有当大多数分片可用时才允许写操作。但即使大多数可用，也可能存在因为网络等原因导致写入副本失败，这样该副本被认为故障，分片将会在一个不同的节点上重建。
   ● 对于读操作，可以设置 replication 为 sync(默认)，这使得操作在主分片和副本分片都完成后才会返回；如果设置 replication 为 async 时，也可以通过设置搜索请求参数_preference 为 primary 来查询主分片，确保文档是最新版本。
   为什么倒排索引被设计成不可变的？
   倒排索引的不可变性，这点主要是因为 Elasticsearch 的底层是基于 Lucene，而在 Lucene 中提出了按段搜索的概念，将一个索引文件拆分为多个子文件，则每个子文件叫作段，每个段都是一个独立的可被搜索的数据集，并且段具有不变性，一旦索引的数据被写入硬盘，就不可再修改。
   在底层采用了分段的存储模式，使它在读写时几乎完全避免了锁的出现，大大提升了读写性能。说到这，你们可能会想到 ConcurrentHashMap 的分段锁 的概念，其实原理有点类似。段的概念提出主要是因为：在早期全文检索中为整个文档集合建立了一个很大的倒排索引，并将其写入磁盘中。如果索引有更新，就需要重新全量创建一个索引来替换原来的索引。这种方式在数据量很大时效率很低，并且由于创建一次索引的成本很高，所以对数据的更新不能过于频繁，也就不能保证时效性。
   Elasticsearch 中的倒排索引被设计成不可变的，有以下几个方面优势：
   ● 不需要加锁，提升了并发性能，不需要担心多进程同时修改数据的问题。
   ● 查询出的数据就能一直保存在缓存中，比如过滤查询filter就使用了缓存。由于其不变性，只要文件系统缓存中还有足够的空间，那么大部分读请求会直接请求内存，而不会命中磁盘。这提供了很大的性能提升。
   ● 其它缓存(像filter缓存)，在索引的生命周期内始终有效。它们不需要在每次数据改变时被重建，因为数据不会变化。
   ● 写入单个大的倒排索引允许数据被压缩，减少磁盘 I/O 和 需要被缓存到内存的索引的使用量。
   每一个段本身都是一个倒排索引，但索引在 Lucene 中除表示所有段的集合外，还增加了提交点的概念。为了提升写的性能，Lucene并没有每新增一条数据就增加一个段，而是采用延迟写的策略，每当有新增的数据时，就将其先写入内存中，然后批量写入磁盘中。若有一个段被写到硬盘，就会生成一个提交点，提交点就是一个列出了所有已知段和记录所有提交后的段信息的文件。
   不可变，那么如何更新索引？
   https://blog.csdn.net/lijingjingchn/article/details/117033008
   ES 的解决方法就是：用更多的索引。什么意思？就是原来的索引不变，我们对新的文档再创建一个索引。为了保留索引不变性，ES 会通过增加新的补充索引来反映新近的修改，而不是直接重写整个倒排索引。每一个倒排索引都会被轮流查询到（从最早的开始），查询完后再对结果进行合并。对于修改的场景来说，同一个文档这时磁盘中同时会有两个索引数据一个是原来的索引，另一个是修改之后的索引。当一个文档被更新时，旧版本文档被标记删除，文档的新版本被索引到一个新的段中。 可能两个版本的文档都会被一个查询匹配到，但被删除的那个旧版本文档在结果集返回前就已经被移除。
   ES更新和删除文档流程？
   由于ES中的文档是不可变的，因此不能被删除或者改动以展示其变更。磁盘上的每个段都有一个对应的.del文件。当删除请求发送后，文档并没有真的被删除，而是在.del文件中被标记为删除。该文档依然能够匹配查询，但是会在结果中被过滤掉。当段合并时，在.del文件中被标记为删除的文档将不会被写入新段。在新的文档被创建的时候，ES会为该文档指定一个版本号，当执行更新操作的时候，旧版本的文档在.del文件中被标记为三处，新本的文档会被索引到一个新段。旧版本的文档依然能匹配查询，但是会在结果中被过滤掉。
   ES索引文档的流程？
   客户端向服务器发出索引文档的请求，会根据路由算法，找到主分片的位置，往里面索引数据，同时为了保证数据的一致性，会对副本进行数据同步。用户发送索引请求后，首先会在内存当中创建一个索引，并且将索引在内存中形成一个分段对象。同时为了防止数据出现问题，会将操作存在内存中的日志。并且每隔一秒钟会将内存中的segement刷到文件系统的缓存中。每隔一段时间，会将文件系统缓存中的segment刷入硬盘中。
   ES搜索的流程？
1. 搜索被执行成为一个两阶段过程，我们称之为Query Then Fetch；
2. 在初始查询阶段，查询会广播到索引中每一个分片副本。每个分片在本地执行搜索并构建一个匹配文档的大小为from+size的优先队列。PS：在搜索的时候是会查询File system Cache的，但是有部分数据还在Memory Buffer，所以搜索是近实时的。
3. 每个分片返回各自优先队列中所有文档的ID和排序值给协调节点，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表。
4. 接下来就是取回阶段，协调节点辨别出那些文档需要被取回并向相关的分片提交多个GET请求。每个分片加载并丰富文档，如果有需要的话，接着返回文档给协调节点。一旦所有的文档都被取回了，协调节点返回结果给客户端。
   ES的master选举流程？
1. ES的选举是ZenDiscovery模块负责的，主要包含Ping和Unicast（单播模块包含一个主机列表以控制哪些节点需要ping通）这个两个部分。
2. 对所有可以成为master的节点根据nodeID字典排序，每次选举每个节点，都把自己所知道的节点排一次序，然后选出第一个节点，暂定认为他为master节点。
3. 如果对某个节点的投票数达到一定的值并且该节点自己也选举自己，那该节点就是master。否则重新选举一直到满足上述条件。
4. master结点的职责主要包括集群、节点和索引的管理，不负责文档级别的管理；data节点可以关闭http功能。
   ES 脑裂问题怎么处理？
   脑裂可能的成因:
   ● 网络问题：集群间的网络延迟导致一些节点访问不到master，认为master挂掉了从而选举出新的master，并对master上的分片和副本标红，分配新的主分片
   ● 节点负载：主节点的角色既为master又为data，访问量较大时可能会导致ES停止响应造成大面积延迟，此时其他节点得不到主节点的响应认为主节点挂掉了，会重新选取主节点。
   ● 内存回收：data节点上的ES进程占用的内存较大，引发JVM的大规模内存回收，造成ES进程失去响应。
   解决方案：
   ● 角色分离：不要把主节点同时设为数据节点，数据节点是没有选举资格的。
   ● 提高选举条件：discovery.zen.minimum_master_nodes:3 该参数是用于控制选举行为发生的最小集群主节点数量。当备选主节点的个数大于等于该参数的值，且备选主节点采用选举资格。
   ● 减少误判：节点之间的 keepalive 是通过 ZenDiscocery 机制实现的，可以适当调大节点状态的响应时间(discovery.zen.ping_timeout)参数，如果master在该响应时间的范围内没有做出响应应答，才判断该节点已经挂掉了，调大参数可适当减少误判。
   ES 怎么进行参数调优？
   关于ES的参数，大部分情况下是不需要进行调优，如果出现性能问题，最好的办法是安排更合理的分片布局或者增加节点数量。
   参数配置：定制一下JVM的堆内存空间
   集群部署：
   ● 调整 ES 的一些配置参数，比如 path.data 目录尽量使用固态硬盘（SSD）；
   ● 对于节点的配置，可以选择机器性能比较好的作为主节点，硬盘空间大的作为数据节点；
   更合理的分片布局，让分片和对应的副本尽量在同一个机房，减少在网络上的延迟（当然，通常为了ES容灾，会将分片和副本部署在不同位置以保证数据的高可用）
   ES 数据多了怎么办，如何调优/部署：
   索引数据的规划，应在前期做好规划，正所谓“设计先行，编码在后”，这样才能有效的避免突如其来的数据激增导致集群处理能力不足引发的线上客户检索或者其他业务受到影响。
   ●  动态索引：基于模板+时间+rollover api 滚动创建索引，举例：设计阶段定义：blog 索引的模板格式为：blog_index_时间戳的形式，每天递增数据。这样做的好处：不至于数据量激增导致单个索引数据量非常大，接近于上线 2 的32 次幂-1，索引存储达到了 TB+甚至更大。一旦单个索引很大，存储等各种风险也随之而来，所以要提前考虑及早避免。
   ● 存储层面：冷热数据分离存储，热数据（比如最近 3 天或者一周的数据），其余为冷数据。对于冷数据不会再写入新数据，可以考虑定期 force_merge 加 shrink 压缩操作，节省存储空间和检索效率。
   ● 部署层面：一旦之前没有规划，这里就属于应急策略。结合 ES 自身的支持动态扩展的特点，动态新增机器的方式可以缓解集群压力，注意：如果之前主节点等规划合理，不需要重启集群也能完成动态新增的。
   ES 性能优化方式有哪些？
   写入数据：
   ● 尽量使用 bulk 进行批量写入
   ● 增加 flush 的时间间隔，以降低数据写入磁盘的频率，减少磁盘IO
   ● 通过降低 refresh 频率减少segment文件的创建，也能减少 segment 文件的合并次数（但同时也降低了ES搜索的实时性）
   搜索数据：
   ● 尽量避免深翻页，单页数据量也不宜过大，需要时可以使用 scroll 滚动查询 或者 search after 的方式
   ● 查询时通过 include 和 exclude 来指定哪些字段需要返回，减少不必要的硬盘和网络IO
   ● 文档大时，可以考虑使用 ES 加 MySQL 的方式进行查询
   ● 在不需要某些字段的相关性评分时，用 filter 代替 query
   索引映射：
   ● 选着合适的分片数量和副本数量
   ● 字段使用占用空间更小的类型，能用 keyword 就不用 text
   ● 字段的 index 属性只用于需要建立倒排索引的情景下配置为 true
   ● 字段的 store 属性只用于需要建立倒排但无需存储的情况下配置为 false，节省存储空间
   ● keyword类型字段的 ignore_above 属性用于设置超过指定字符后，超出的部分不能被索引或者存储
   ● 冷热分离架构是Elasticsearch的经典架构之一，使用该架构用户可以在保证热数据良好读写性能的同时，仍可以存储海量的数据，极大地丰富了ES的应用场景，解决了用户的成本问题。
   重建索引：
   当数据量很大的时候，可以先将刷新时间间隔 refresh_intervals 设置为-1（即不刷新），降低 refresh 频率减少 segment文件的创建，也能减少 segment 文件的合并次数；将 number_of_replicas 副本数设置为 0，因为副本数是可以动态调整，可以等索引重建完成之后，再配置副本数量，有助于提升重建索引的效率。另外记得删除旧索引以释放磁盘空间。
   ES 查询队列满，怎么解决？
   队列大小一般不需要调整，队列满的原因是请求处理不过来，调大之后解决不了根本问题。
1. 首先看看分片在节点上的分布是否均匀，分布得不均匀容易出现队列打满的情况，可以通过重新规划分片，更加充分地利用节点的资源；
2. 如果分片均衡没有问题，可以把慢日志打开，查看是否有大量查询比较慢的请求，如果有的话，需要联系业务侧分析该类请求；
3. 如果以上两点做了之后，队列还是会满，那说明节点资源不足以支撑请求量，需要考虑扩容。
   分词器了解吗？用过哪些分词器？
   分词器(Analyzer)一般由三种组件构成：
   ● character filter (字符过滤器)：在一段文本分词之前，先进行预处理，比如说最常见的就是过滤html标签
   ● tokenizers (分词器)：默认情况下，英文分词根据空格将单词分开；中文分词按单字隔开，也可以采用机器学习算法来分词
   ● token filters (Token过滤器)：将切分的单词进行加工，大小写转换，去掉停用词(例如“a”、“and”、“the”等等 )，加入同义词(例如同义词像“jump”和“leap”)
   内置分词器：
   ● Standard Analyzer - 默认分词器，英文按单词词切分，并小写处理，中文部分切割为一个一个的汉字。
   ● Simple Analyzer - 按照单词切分，小写处理，中文部分不会切割为一个一个的汉字。
   ● Stop Analyzer - 在停止词进行分割，中文部分不会切割为一个一个的汉字
   ● Whitespace Analyzer - 按照空格切分，不转小写
   ● Keyword Analyzer - 不分词，直接将输入当作输出
   中文分词器：
   ● SmartCN ⼀个简单的中⽂、中英⽂混合⽂本的分词器
   ● IK 比smartCN更智能、更友好，推荐
   ○ 扩展词：就是有些词并不是关键词，但是也希望被ES用来作为检索的关键词，可以将这些词加入扩展词典
   ○ 停用词：就是有些关键词，我们并不想让他被检索到，可以放入停用词典中

translog 是啥？
1. 引入translog
   当一个文档写入Lucence后是存储在内存中的，即使执行了refresh操作仍然是在文件系统缓存中，如果此时服务器宕机，那么这部分数据将会丢失。为此ES增加了translog， 当进行文档写操作时会先将文档写入Lucene，然后写入一份到translog，写入translog是落盘的(如果对可靠性要求不是很高，也可以设置异步落盘，可以提高性能，由配置index.translog.durability和index.translog.sync_interval控制)，这样就可以防止服务器宕机后数据的丢失。由于translog是追加写入，因此性能比较好。与传统的分布式系统不同，这里是先写入Lucene再写入translog，原因是写入Lucene可能会失败，为了减少写入失败回滚的复杂度，因此先写入Lucene.


2. flush操作
   另外每30分钟或当translog达到一定大小(由index.translog.flush_threshold_size控制，默认512mb), ES会触发一次flush操作，此时ES会先执行refresh操作将buffer中的数据生成segment，然后调用lucene的commit方法将所有内存中的segment fsync到磁盘。此时lucene中的数据就完成了持久化，会清空translog中的数据(6.x版本为了实现sequenceIDs,不删除translog)

MySQL
数据库三范式是什么？
第一范式（1NF）列不可再分：列的具有原子性，列的信息不能分解。只要数据库是关系型数据库，就应该满足第一范式。
通俗的解释就是：数据库表的每一列都是不可分割的原子数据项，而不能是集合、数组、记录等非原子数据项。
第二范式（2NF）属性完全依赖于主键：第二范式要求数据库表中的每个实例或行必须可以被惟一地区分。为实现区分通常需要我们设计一个主键来实现(这里的主键不包含业务逻辑)。比如有两个主键，不能存在这样的属性，它只依赖于其中一个主键，这就是不符合第二范式。通俗理解是任意一个字段都只依赖表中的同一个字段，也就是说在一个数据库表中，一个表中只能保存一种数据，不可以把多种数据保存在同一张数据库表中。
第三范式（3NF）不存在对非主键列的传递依赖：确保数据表中的每一列数据都和主键直接相关，而不能间接相关，不能存在传递依赖。

说一下 MySQL 常用的存储引擎？
MySQL 常用的存储引擎有：InnoDB、MyISAM 还有 Memory 等存储引擎。目前 InnoDB 是最新版MySQL数据库的默认存储引擎。
InnoDB 和 MyISAM 存储引擎的区别？
● 事务：MyISAM 存储引擎不支持事务，但是 InnoDB 支持。并且 InnoDB 的默认自动提交事务。
● 锁：InnoDB 存储引擎支持数据行锁定，MyISAM 不支持行锁定，只支持锁定整个表。
● 索引：InnoDB 和 MyISAM 存储引擎的索引都使用 B+ 树的数据结构，但是 InnoDB 使用的是聚集索引的方式，而MyISAM 使用的是非聚集索引的方式（从表格文件上来解释：InnoDB的索引和数据用的同一个文件才存储，而MyISAM 将索引和数据本身分开进行存储），补充：旧的版本中 innoDB 不支持全文索引，而 MyISAM 支持。
聚集索引和非聚集索引的区别？
首先，在MySQL数据库中聚集索引和非聚集索引都是 B+树的索引结构。
简单来说，聚集索引就是在B+树的叶子节点中存放完整的数据行信息，而非聚集索引与之相反。拿innoDB存储引擎来说，它的主键索引中的叶子节存放了完整的数据行信息，因此innoDB存储引擎使用的聚集索引；而对于MyISAM存储引擎而言，它的主键索引中的叶子节存放的是数据所在的地址信息，因此MyISAM存储引擎使用的是非聚集索引。
了解MySQL的索引吗？
● 索引的目的：索引在MySQL是一个数据结构，是为了提高查询数据的效率、提高检索的速度。（类似于字典中的目录）
● 索引的分类：
○ 单列索引：主键索引，唯一索引，普通索引。
○ 组合索引
○ 全文索引
● 索引的结构：在MySQL中常用两种索引结构：B+树 和 Hash，两种算法检索方式不一样，对查询的作用也不一样，常用的 MyISM 和 InnoDB 存储引擎使用的是 B+树 索引。
B 树和B+树的区别？
● 节点结构不同：B+树只有叶子节点存放了完整的数据，查询时间复杂度固定为 O(log n) ，而B树每个节点有存放了索引和数据，查询的时间复杂度在 O(1) ~ O(log n)之间（和 Key在树中的的位置有关）；
● 范围查找：B+树叶节点两两相连可大大增加区间访问性，提高了范围查询的能力；
● B+树更适合外部存储：由于内节点无 data 域，每个节点能索引的范围更大更精确，由于B树的节点都存了key和data，而B+树只有叶子节点存data，非叶子节点都只是索引值，没有实际的数据，这就时B+树在一次IO里面，能读出的索引值更多。从而减少查询时候需要的IO次数！
空间局部性原理：如果一个存储器的某个位置被访问，那么将它附近的位置也会被访问。
B+树索引和Hash索引的区别？
● Hash是k,v形式，通过一个散列函数，能够根据key快速找到value
● 哈希索引就是采用一定的hash算法，把键值换成新的哈希值，检索时不需要类似B+树那样从根节点到叶子节点逐级查找，只需要一次hash算法即可立即定位到相应的位置，速度非常快。
●  缺点: 因为底层数据结构是散列的，无法进行比较大小，不能进行范围查找
B+树索引和hash索引的明显区别：
1、如果是等值查询，那么hash索引有明显的优势，因为只需要经过一次算法即可找到相应的键值；当然了，这个键值是唯一的，如果不唯一，则需要先找到下标位置再链式查找。
2、从示意图可以知道，hash索引无法支持范围查询，因为原先是有序的键值，但是经过hash算法后，有可能变成不连续的，就没有办法利用索引完成范围查询检索数据。
3、同样，hash索引也没办法利用索引完成排序，以及like `xxx%`这样的模糊查询（范围查询）。
4、hash索引也不支持多列联合索引的最左前缀匹配规则。
5、B+树索引的关键字检索效率比较平均，不像B树那样波动幅度大，在有大量重复键的情况下，hash索引的效率也是极低的，因为存在hash碰撞问题。
了解 MySQL 的事务吗？
● 定义：事务，简单来说，是逻辑上的一组操作，要么全都执行，要么全都不执行。理论上说，事务必须同时满足四个特性，也就是通常所说的事务的 ACID 特性 （原子性、一致性、隔离性 和 持久性）。
MVCC
多版本并发控制，是为了在读取数据时不加锁来提高读取效率和并发性的一种手段。mvcc解决的就是读写时的线程安全问题，线程不用去争抢读写锁。mvcc所提到的读是快照读，也就是普通的select语句。快照读在读写时不用加锁，不过可能会读到历史数据。
实现原理：基于undolog、版本链、readview。

在mysql存储的数据中，除了我们显式定义的字段，mysql会隐含的帮我们定义几个字段。
● trx_id: 事务id，每进行一次事务操作，就会自增1。
● roll_pointer: 回滚指针，用于找到上一个版本的数据，结合undolog进行回滚。
Read View
当我们用select读取数据时，这一时刻的数据会有很多个版本（例如上图有四个版本），但我们并不知道读取哪个版本，这时就靠readview来对我们进行读取版本的限制，通过readview我们才知道自己能够读取哪个版本。readview快照中主要包括以下这些字段：
● m_ids：表示在生成readview时，当前系统中活跃的读写事务id列表；
● min_trx_id：表示在生成readview时，当前系统中活跃的读写事务中最小的事务id，也就是m_ids中最小的值；
● max_trx_id：表示生成readview时，系统中应该分配给下一个事务的id值；
● creator_trx_id：表示生成该readview的事务的事务id；
mvcc如何实现RC和RR的隔离级别?
● RC的隔离级别下，每个快照读都会生成并获取最新的readview。
● RR的隔离级别下，只有在同一个事务的第一个快照读才会创建readview，之后的每次快照读都使用的同一个readview，所以每次的查询结果都是一样的。

事务的隔离级别有哪些？
● 读取未提交（READ UNCOMMITTED）：一个事务能够读取其他事务未提交的数据。隔离级别最低，没有安全性可言，基本不会使用。
● 读取提交（READ COMMITTED）：一个事务只能读取其他事务已提交的数据。这是大多数数据库的默认隔离级别，如 Oracle, SQLserver。
● 可重复读取（REPEATABLE READ）：一个事务中进行多次相同的读取操作，得到的结果是一样的。这是Mysql数据库的默认隔离级别。
● 串行化（SERIALIZABLE）：事务执行的时候不允许别的事务并发执行，而是完全串行化的执行，只要存在读就禁止写，但可以同时读，消除了幻读。这是事务隔离的最高级别，虽然最安全，但是效率太低，一般不会用。
为什么很多公司选择mysql的事务隔离级别是RC？
从以下两个方面来看，
1. 使用RR事务隔离级别，能避免幻读，但是由于引入间隙锁导致加锁的范围可能扩大，从而会影响并发，还容易造成死锁，因为间隙锁和间隙锁是不冲突的；
2. 在大多数业务场景下，事务隔离级别RC基本上能满足业务需求，幻读出现的机率较少；

从够用的角度来看，选择RC隔离级别是可以的。
事务隔离级别与在该级别下可能会发生的问题
● 脏读：一个事务读取了另外一个事务未提交的数据
● 不可重复读：在一个事务内读取表中的数据，多次读取结果不一致。（在某些业务场合下，不一定是错误）
● 幻读：同一事务中，用同样的操作读取两次，得到的记录数量不相同。
事务隔离级别	脏读	不可重复读	幻读
读取未提交（read-uncommitted）	✔	✔	✔
读取提交（read-committed）		✔	✔
可重复读（repeatable-read）			✔
串行化（serializable）			
不可重复读和幻读的区别是什么？
● 前者是指读到了已经提交的事务的更改数据（针对 update）
● 后者是指读到了其他已经提交事务的新增数据（针对 insert）。
● 对于这两种问题解决采用不同的办法，防止读到更改数据，只需对操作的数据添加行级锁，防止操作中的数据发生变化；而防止读到新增数据，往往需要添加表级锁，将整张表锁定，防止新增数据。
● 当隔离级别设置为串行化，强制事务串行执行，可以完全避免了前面说的幻读的问题。
可重复读隔离级别，完全解决幻读了吗？
可重复读隔离级别，根据不同的查询方式，分别提出了避免幻读的方案：
● 针对快照读（普通 select 语句），是通过 MVCC 方式解决了幻读。
● 针对当前读（select ... for update 等语句），是通过 next-key lock（记录锁+间隙锁）方式解决了幻读
可重复读隔离级别下虽然很大程度上避免了幻读，但是还是没有能完全解决幻读。LINK

数据库死锁了解吗？
https://www.yuque.com/zyldgd/other/pnt40t#K6t0k
https://xiaolincoding.com/mysql/lock/show_lock.html#%E6%80%BB%E7%BB%93
● 概念：死锁是指多个事务在执行过程中，因争夺锁资源而造成的一种相互等待的僵持状态，若无外力作用，事务将没法进行下去。
● 示例：
○ 不同表-相同记录锁冲突
○ 相同表-相同记录锁冲突
● 如何避免死锁：避免死锁
● 两个事务即使生成的间隙锁的范围是一样的，也不会发生冲突，因为间隙锁目的是为了防止其他事务插入数据，因此间隙锁与间隙锁之间是相互兼容的。
● 在执行插入语句时，如果插入的记录在其他事务持有间隙锁范围内，插入语句就会被阻塞，因为插入语句在碰到间隙锁时，会生成一个插入意向锁，然后插入意向锁和间隙锁之间是互斥的关系。
● 如果两个事务分别向对方持有的间隙锁范围内插入一条记录，而插入操作为了获取到插入意向锁，都在等待对方事务的间隙锁释放，于是就造成了循环等待，满足了死锁的四个条件：互斥、占有且等待、不可强占用、循环等待，因此发生了死锁。
如何避免死锁？
一般来说MySQL通过回滚帮我们解决了不少死锁的问题了，但死锁是无法完全避免的，可以通过以下的经验参考，来尽可能少遇到死锁：
○ 以固定的顺序访问表和行：比如对两个事务批量更新的情形，简单方法是对id列表先排序，后执行，这样就避免了交叉等待锁的情形；将两个事务的sql顺序调整一致，也能避免死锁。
○ 大事务拆小：大事务更容易发生死锁，如果业务允许，将大事务拆小。
○ 直接使用更大颗粒的锁：在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁概率。
○ 降低隔离级别：如果业务允许，将隔离级别调低也是较好的选择，比如将隔离级别从RR调整为RC，可以避免掉很多因为间隙锁造成的死锁。
○ 为表添加合理的索引：如果不走索引将会为表的每一行记录添加上锁，死锁的概率大大增大。
如何定位死锁成因？
1.通过应用业务日志定位到问题代码，找到相应的事务对应的sql
因为死锁被检测到后会回滚，这些信息都会以异常反应在应用的业务日志中，通过这些日志我们可以定位到相应的代码，并把事务的sql给梳理出来。  此外，我们根据日志回滚的信息发现在检测出死锁时这个事务被回滚。
2.确定数据库隔离级别
执行select @@global.tx_isolation，可以确定数据库的隔离级别，我们数据库的隔离级别是RC，这样可以很大概率排除gap锁造成死锁的嫌疑。
3.找DBA执行下 show engine innodb status 看看最近死锁的日志
如何优化索引？
索引优化
● 前缀索引优化；
● 覆盖索引优化；
● 主键索引最好是自增的；
● 防止索引失效；
mysql 索引失效的情况有哪些？
● 当我们使用左或者左右模糊匹配的时候，也就是 like %xx 或者 like %xx% 这两种方式都会造成索引失效；
● 当我们在查询条件中对索引列做了计算、函数、类型转换操作，这些情况下都会造成索引失效；
● 联合索引要能正确使用需要遵循最左匹配原则，也就是按照最左优先的方式进行索引的匹配，否则就会导致索引失效。
● 在 WHERE 子句中，如果在 OR 前的条件列是索引列，而在 OR 后的条件列不是索引列，那么索引会失效。
如何进行语句分析？
检查sql语句的执行计划，如下图所示：

其参数有：
● possible_keys 字段表示可能用到的索引；
● key 字段表示实际用的索引，如果这一项为 NULL，说明没有使用索引；
● key_len 表示索引的长度；
● rows 表示扫描的数据行数。
● type 表示数据扫描类型，描述了找到所需数据时使用的扫描方式是什么，常见扫描类型的执行效率从低到高的顺序为：
○ All（全表扫描）：all 是最坏的情况，因为采用了全表扫描的方式；
○ index（全索引扫描）：index 和 all 差不多，只不过 index 对索引表进行全扫描，这样做的好处是不再需要对数据进行排序，但是开销依然很大；
○ range（索引范围扫描）：一般在 where 子句中使用 < 、>、in、between 等关键词，只检索给定范围的行，属于范围查找；
○ ref（非唯一索引扫描）：虽然使用了索引，但该索引列的值并不唯一，有重复；
○ eq_ref（唯一索引扫描）：使用主键或唯一索引时产生的访问方式，通常使用在多表联查中。比如，对两张表进行联查，关联条件是两张表的 user_id 相等，且 user_id 是唯一索引，那么使用 EXPLAIN 进行执行计划查看的时候，type 就会显示 eq_ref；
○ const（结果只有一条的主键或唯一索引扫描）：使用了主键或者唯一索引与常量值进行比较；
除了关注 type，我们也要关注 extra 显示的结果，这里说几个重要的参考指标：
● Using filesort ：当查询语句中包含 group by 操作，而且无法利用索引完成排序操作的时候， 这时不得不选择相应的排序算法进行，甚至可能会通过文件排序，效率是很低的，所以要避免这种问题的出现。
● Using temporary：使了用临时表保存中间结果，MySQL 在对查询结果排序时使用临时表，常见于排序 order by 和分组查询 group by。效率低，要避免这种问题的出现。
● Using index：所需数据只需在索引即可全部获得，不须要再到表中取数据，也就是使用了覆盖索引，避免了回表操作，效率不错。
undo log、redo log、binlog 有什么用？
● undo log（回滚日志）：是 Innodb 存储引擎层生成的日志，实现了事务中的原子性，主要用于事务回滚和 MVCC。
● redo log（重做日志）：是 Innodb 存储引擎层生成的日志，实现了事务中的持久性，主要用于掉电等故障恢复；
● binlog （归档日志）：是 Server 层生成的日志，主要用于数据备份和主从复制；
为什么需要 undo log？
undo log 是一种用于撤销回退的日志。在事务没提交之前，MySQL 会先记录更新前的数据到 undo log 日志文件里面，当事务回滚时，可以利用 undo log 来进行回滚。在发生回滚时，就读取 undo log 里的数据，然后做原先相反操作。
另外，undo log 还有一个作用，实现 MVCC（多版本并发控制）。MVCC 是通过 ReadView + undo log 实现的。undo log 为每条记录保存多份历史数据，MySQL 在执行快照读（普通 select 语句）的时候，会根据事务的 Read View 里的信息，顺着 undo log 的版本链找到满足其可见性的记录。
为什么需要 Buffer Pool？
MySQL 的数据都是存在磁盘中的，那么我们要更新一条记录的时候，得先要从磁盘读取该记录，然后在内存中修改这条记录。那修改完这条记录是选择直接写回到磁盘，还是选择缓存起来呢？当然是缓存起来好，这样下次有查询语句命中了这条记录，直接读取缓存中的记录，就不需要从磁盘获取数据了。为此，Innodb 存储引擎设计了一个缓冲池（Buffer Pool），来提高数据库的读写性能。
InnoDB 会把存储的数据划分为若干个「页」，以页作为磁盘和内存交互的基本单位，一个页的默认大小为 16KB。因此，Buffer Pool 同样需要按「页」来划分。
Buffer Pool 缓存什么？
在 MySQL 启动的时候，InnoDB 会为 Buffer Pool 申请一片连续的内存空间，然后按照默认的16KB的大小划分出一个个的页， Buffer Pool 中的页就叫做缓存页。此时这些缓存页都是空闲的，之后随着程序的运行，才会有磁盘上的页被缓存到 Buffer Pool 中。Buffer Pool 除了缓存「索引页」和「数据页」，还包括了 Undo 页，插入缓存、自适应哈希索引、锁信息等等。
为什么需要 redo log ？
Buffer Pool 是提高了读写效率没错，但是问题来了，Buffer Pool 是基于内存的，而内存总是不可靠，万一断电重启，还没来得及落盘的脏页数据就会丢失。
为了防止断电导致数据丢失的问题，当有一条记录需要更新的时候，InnoDB 引擎就会先更新内存（同时标记为脏页），然后将本次对这个页的修改以 redo log 的形式记录下来，这个时候更新就算完成了。后续，InnoDB 引擎会在适当的时候，由后台线程将缓存在 Buffer Pool 的脏页刷新到磁盘里，这就是 WAL （Write-Ahead Logging）技术。在事务提交时，只要先将 redo log 持久化到磁盘即可，可以不需要等到将缓存在 Buffer Pool 里的脏页数据持久化到磁盘。当系统崩溃时，虽然脏页数据没有持久化，但是 redo log 已经持久化，接着 MySQL 重启后，可以根据 redo log 的内容，将所有数据恢复到最新的状态。
redo log 要写到磁盘，数据也要写磁盘，为什么要多此一举？
写入 redo log 的方式使用了追加操作， 所以磁盘操作是顺序写，而写入数据需要先找到写入位置，然后才写到磁盘，所以磁盘操作是随机写。磁盘的「顺序写 」比「随机写」 高效的多，因此 redo log 写入磁盘的开销更小。
redo log 什么时候刷盘？
缓存在 redo log buffer 里的 redo log 还是在内存中，它什么时候刷新到磁盘？主要有下面几个时机：
● MySQL 正常关闭时；
● 当 redo log buffer 中记录的写入量大于 redo log buffer 内存空间的一半时，会触发落盘；
● InnoDB 的后台线程每隔 1 秒，将 redo log buffer 持久化到磁盘。
● 每次事务提交时都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘（这个策略可由 innodb_flush_log_at_trx_commit 参数控制，下面会说）。
为什么需要 bin log ？
undo log 和 redo log 这两个日志都是 Innodb 存储引擎生成的。MySQL 在完成一条更新操作后，Server 层还会生成一条 binlog，等之后事务提交的时候，会将该事物执行过程中产生的所有 binlog 统一写 入 binlog 文件。binlog 文件是记录了所有数据库表结构变更和表数据修改的日志，不会记录查询类的操作。
最开始 MySQL 里并没有 InnoDB 引擎，MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力，binlog 日志只能用于归档。而 InnoDB 是另一个公司以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用 redo log 来实现 crash-safe 能力。
binlog 什么时候刷盘？
事务执行过程中，先把日志写到 binlog cache（Server 层的 cache），事务提交的时候，再把 binlog cache 写到 binlog 文件中。MySQL 给 binlog cache 分配了一片内存，每个线程一个，参数 binlog_cache_size 用于控制单个线程内 binlog cache 所占内存的大小。如果超过了这个参数规定的大小，就要暂存到磁盘。
redo log 和 binlog 有什么区别？
适用对象不同：
● binlog 是 MySQL 的 Server 层实现的日志，所有存储引擎都可以使用；
● redo log 是 Innodb 存储引擎实现的日志；
写入方式不同：
● binlog 是追加写，写满一个文件，就创建一个新的文件继续写，不会覆盖以前的日志，保存的是全量的日志。
● redo log 是循环写，日志空间大小是固定，全部写满就从头开始，保存未被刷入磁盘的脏页日志。
用途不同：
● binlog 用于备份恢复、主从复制；
● redo log 用于掉电等故障恢复。
主从复制是怎么实现？
MySQL 的主从复制依赖于 binlog ，也就是记录 MySQL 上的所有变化并以二进制形式保存在磁盘上。复制的过程就是将 binlog 中的数据从主库传输到从库上。这个过程一般是异步的，也就是主库上执行事务操作的线程不会等待复制 binlog 的线程同步完成。
具体详细过程如下：
1. MySQL 主库在收到客户端提交事务的请求之后，会先写入 binlog，再提交事务，更新存储引擎中的数据，事务提交完成后，返回给客户端“操作成功”的响应。
2. 从库会创建一个专门的 I/O 线程，连接主库的 log dump 线程，来接收主库的 binlog 日志，再把 binlog 信息写入 relay log 的中继日志里，再返回给主库“复制成功”的响应。
3. 从库会创建一个用于回放 binlog 的线程，去读 relay log 中继日志，然后回放 binlog 更新存储引擎中的数据，最终实现主从的数据一致性。
   MySQL 主从复制还有哪些模型？
   ● 同步复制：MySQL 主库提交事务的线程要等待所有从库的复制成功响应，才返回客户端结果。
   ● 异步复制（默认模型）：MySQL 主库提交事务的线程并不会等待 binlog 同步到各从库，就返回客户端结果。这种模式一旦主库宕机，数据就会发生丢失。
   ● 半同步复制：MySQL 5.7 版本之后增加的一种复制方式，介于两者之间，事务线程不用等待所有的从库复制成功响应，只要一部分复制成功响应回来就行，比如一主二从的集群，只要数据成功复制到任意一个从库上，主库的事务线程就可以返回给客户端。这种半同步复制的方式，兼顾了异步复制和同步复制的优点，即使出现主库宕机，至少还有一个从库有最新的数据，不存在数据丢失的风险。
   mysql 节点多大？
   所以，叶子节点是个链表，理论上可以无限大，但是非叶子节点就是一个16k大小的page，所以对于一棵树能存多少数据，主要就看非叶节点能存下多少个[主键ID+指针]了。
   ps:一个节点有多少个[主键ID+指针]，其实就是m树的m是多大了。
   下面以一个高度=2，且主键ID是一个bigint(8字节)来分析可以存下多少数据(这个假设是有意义的，在绝大部分主键id都是一个自增的bigint)。
   Innodb中一个指针是6字节长度。所以[主键ID+指针]总共就占14字节。所以一个16K大小的节点可以存下的[主键ID+指针]个数=16K/14=16384/14=1170，也就是说一个高度=2的B+树可以放下1170个叶子节点，即1170个用于存放行数据的page，即可以存放的行数据的大小=1170*16K=18720K=1.8M，准确的说这是树上的，还有很多不在树上的，所以实际能放下的数据不止1.8M。
   ps:一个page内还有一些其他的数据，如next指针，LSN等，所以说一个page的16K不完全都拿来存行数据的。
   如果下面我们分析高度=3，即有两层非叶子节点的B+树，能存放多少数据。根节点的16k的page可以存放16k/14=1170个[主键ID+指针]，即第二层就可以有1170个page。所以总共树上可以放的叶子节点的个数=1170*1170=1368900，所以能放下的数据=1368900*16K=21902400K=21G。同理，因为不是所有的行数据都在树上，所以高度=3的B+树不止放下21G的数据的。
   再来，看下高度=4的，那么树上可以存下的叶子节点=1170*1170*1170=1601613000个，所以能存下的数据=1601613000*16K=25.6T。同理，实际存下的数据是可以不止这个量的。
   Redis
   https://xiaolincoding.com/redis/base/redis_interview.html#%E8%AE%A4%E8%AF%86-redis
   常见的 Redis 数据类型是怎么实现？
   数据类型底层实现

Redis 是单线程吗？
通常所说的 Redis 单线程是指网络请求模块和数据操作模块是单线程的，Redis 本身并不是单线程模型。
在 Redis 6.0 版本之后也只对处理网络请求过程采用了多线程。
● Redis 操作基于内存，绝大多数操作的性能瓶颈不在 CPU
● 单线程模型，避免了不必要的上下文切换和锁竞争带来的性能开销
● 在单线程中使用I/O多路复用技术也能提升Redis的I/O利用率
https://zhuanlan.zhihu.com/p/360335981
虽然 Redis 的主要工作（网络 I/O 和执行命令）一直是单线程模型，但是在 Redis 6.0 版本之后，也采用了多个 I/O 线程来处理网络请求，这是因为随着网络硬件的性能提升，Redis 的性能瓶颈有时会出现在网络 I/O 的处理上。所以为了提高网络 I/O 的并行度，Redis 6.0 对于网络 I/O 采用多线程来处理。但是对于命令的执行，Redis 仍然使用单线程来处理，所以大家不要误解 Redis 有多线程同时执行命令。
之所以 Redis 采用单线程（网络 I/O 和执行命令）那么快，有如下几个原因：
Redis 的大部分操作都在内存中完成，并且采用了高效的数据结构，因此 Redis 瓶颈可能是机器的内存或者网络带宽，而并非 CPU，既然 CPU 不是瓶颈，那么自然就采用单线程的解决方案了；
Redis 采用单线程模型可以避免了多线程之间的竞争，省去了多线程切换带来的时间和性能上的开销，而且也不会导致死锁问题。
Redis 采用了 I/O 多路复用机制处理大量的客户端 Socket 请求，IO 多路复用机制是指一个线程处理多个 IO 流，就是我们经常听到的 select/epoll 机制。简单来说，在 Redis 只运行单线程的情况下，该机制允许内核中，同时存在多个监听 Socket 和已连接 Socket。内核会一直监听这些 Socket 上的连接请求或数据请求。一旦有请求到达，就会交给 Redis 线程处理，这就实现了一个 Redis 线程处理多个 IO 流的效果。
Pipeline 了解吗？
管道（pipeline）可以一次性发送多条命令并在执行完后一次性将结果返回，pipeline 通过减少客户端与 redis 的通信次数来实现降低往返延时时间，而且 Pipeline 实现的原理是队列，而队列的原理是时先进先出，这样就保证数据的顺序性。通俗点：pipeline就是把一组命令进行打包，然后一次性通过网络发送到Redis，同时将执行的结果批量返回。
为什么使用pipeline会更快？
① 打包减少了网络的开销；② OS进程调度，当不使用管道时，Redis处理每个命令之间是有时间空隙的，因此OS很有可能会将Redis进程转换为sleep状态， 然后运行其它程序，而使用pipeline 时，可以提高CPU利用率，Redis空闲的时间没有那么多，因此，这也是 pipeline 速度会更快的 重要原因之一。

Redis 如何实现数据不丢失（持久化机制）？
Redis 共有三种数据持久化的方式：
● AOF 日志：每执行一条写操作命令，就把该命令以追加的方式写入到一个文件里；
● RDB 快照：将某一时刻的内存数据，以二进制的方式写入磁盘；
● 混合持久化方式：Redis 4.0 新增的方式，集成了 AOF 和 RBD 的优点；
大 Key 对 Redis 有哪些影响？
● 对持久化的影响：
○ AOF 有3种刷盘策略(always / everysec / no)，当策略为 always 时，由于每次写操作都会同步刷盘，因此当写入大 key 时，阻塞的时间会比较久，需要等待数据同步到硬盘中。
○ AOF 重写机制和 RDB 快照的过程，都会分别通过 fork() 函数创建一个子进程来处理任务。会有两个阶段会导致阻塞父进程（主线程）：
■ 创建子进程的过程中，由于要复制父进程的页表等数据结构，阻塞的时间跟页表的大小有关，页表越大，阻塞的时间也越长；
■ 创建完子进程后，如果父进程修改了共享数据中的大 Key，就会发生写时复制，这期间会拷贝物理内存，由于大 Key 占用的物理内存会很大，那么在复制物理内存这一过程，就会比较耗时，所以有可能会阻塞父进程。
● 客户端超时：由于 Redis 执行命令是单线程处理，然后在操作大 key 时会比较耗时，那么就会阻塞 Redis，从客户端这一视角看，就是很久很久都没有响应。
● 网络IO阻塞：每次获取大 key 产生的网络流量较大，如果一个 key 的大小是 1 MB，每秒访问量为 1000，那么每秒会产生 1000MB 的流量，这对于普通千兆网卡的服务器来说是灾难性的。
● 工作线程阻塞：如果使用 del 删除大 key 时，会阻塞工作线程，这样就没办法处理后续的命令。
● 内存分布不均：集群模型在 slot 分片均匀情况下，会出现数据和查询倾斜情况，部分有大 key 的 Redis 节点占用内存多。
如何避免大 Key 呢？
最好在设计阶段，就把大 key 拆分成一个一个小 key。或者定时检查 Redis 是否存在大 key （--bigkeys），如果该大 key 是可以删除的，不要使用 DEL 命令删除，因为该命令删除过程会阻塞主线程，而是用 unlink 命令（Redis 4.0+）删除大 key，因为该命令的删除过程是异步的，不会阻塞主线程。
AOF 日志过大，会触发什么机制？
RDB 在执行快照的时候，数据能修改吗？
Redis 数据持久化的方式？
Redis 持久化
● AOF 日志：每执行一条写操作命令，就把该命令以追加的方式写入到一个文件里；
● RDB 快照：将某一时刻的内存数据，以二进制的方式写入磁盘；
AOF 写回策略有几种？
Redis 执行完写操作命令后，会将命令追加到 server.aof_buf 缓冲区；然后通过 write() 系统调用，将 aof_buf 缓冲区的数据写入到 AOF 文件，此时数据并没有写入到硬盘，而是拷贝到了内核缓冲区 page cache，等待内核将数据写入硬盘；具体内核缓冲区的数据什么时候写入到硬盘，由内核决定。Redis 提供了 3 种写回硬盘的策略，控制的就是上面说的第三步的过程。 在 Redis.conf 配置文件中的 appendfsync 配置项可以有以下 3 种参数可填：
● Always，这个单词的意思是「总是」，所以它的意思是每次写操作命令执行完后，同步将 AOF 日志数据写回硬盘；
● Everysec，这个单词的意思是「每秒」，所以它的意思是每次写操作命令执行完后，先将命令写入到 AOF 文件的内核缓冲区，然后每隔一秒将缓冲区里的内容写回到硬盘；
● No，意味着不由 Redis 控制写回硬盘的时机，转交给操作系统控制写回的时机，也就是每次写操作命令执行完后，先将命令写入到 AOF 文件的内核缓冲区，再由操作系统决定何时将缓冲区内容写回硬盘。
AOF 日志过大会触发什么机制？AOF 重写机制了解吗？
AOF 日志是一个文件，随着执行的写操作命令越来越多，文件的大小会越来越大。 如果当 AOF 日志文件过大就会带来性能问题，比如重启 Redis 后，需要读 AOF 文件的内容以恢复数据，如果文件过大，整个恢复的过程就会很慢。所以，Redis 为了避免 AOF 文件越写越大，提供了 AOF 重写机制，当 AOF 文件的大小超过所设定的阈值后，Redis 就会启用 AOF 重写机制，来压缩 AOF 文件。AOF 重写机制是在重写时，读取当前数据库中的所有键值对，然后将每一个键值对用一条命令记录到「新的 AOF 文件」，等到全部记录完后，就将新的 AOF 文件替换掉现有的 AOF 文件。
Redis 过期删除策略是什么？
Redis 是可以对 key 设置过期时间的，因此需要有相应的机制将已过期的键值对删除，而做这个工作的就是过期键值删除策略。Redis 使用的过期删除策略是「惰性删除+定期删除」这两种策略配和使用。
惰性删除：不主动删除过期键，每次从数据库访问 key 时，都检测 key 是否过期，如果过期则删除该 key。
● 优点：因为每次访问时，才会检查 key 是否过期，所以此策略只会使用很少的系统资源，因此，惰性删除策略对 CPU 时间最友好。
● 缺点：如果一个 key 已经过期，而这个 key 又仍然保留在数据库中，那么只要这个过期 key 一直没有被访问，它所占用的内存就不会释放，造成了一定的内存空间浪费。所以，惰性删除策略对内存不友好。
定期删除：每隔一段时间「随机」从数据库中取出一定数量的 key 进行检查，并删除其中的过期key。
● 优点：通过限制删除操作执行的时长和频率，来减少删除操作对 CPU 的影响，同时也能删除一部分过期的数据减少了过期键对空间的无效占用。
● 缺点：难以确定删除操作执行的时长和频率。如果执行的太频繁，就会对 CPU 不友好；如果执行的太少，那又和惰性删除一样了，过期 key 占用的内存不会及时得到释放。
可以看到，惰性删除策略和定期删除策略都有各自的优点，所以 Redis 选择「惰性删除+定期删除」这两种策略配和使用，以求在合理使用 CPU 时间和避免内存浪费之间取得平衡。
Redis 的定期删除的流程：
1. 从过期字典中随机抽取 20 个 key；
2. 检查这 20 个 key 是否过期，并删除已过期的 key；
3. 如果本轮检查的已过期 key 的数量，超过 5 个（20/4），也就是「已过期 key 的数量」占比「随机抽取 key 的数量」大于 25%，则继续重复步骤 1；如果已过期的 key 比例小于 25%，则停止继续删除过期 key，然后等待下一轮再检查。
   Redis 内存淘汰机制有哪些？
   在 Redis 的运行内存达到了某个阀值，就会触发内存淘汰机制，这个阀值就是我们设置的最大运行内存，此值在 Redis 的配置文件中可以找到，配置项为 maxmemory。Redis 内存淘汰策略共有八种，大体分为「不进行数据淘汰」和「进行数据淘汰」两类策略：
   ● 不进行数据淘汰的策略：
   ○ noeviction（Redis3.0之后，默认的内存淘汰策略） ：它表示当运行内存超过最大设置内存时，不淘汰任何数据，而是不再提供服务，直接返回错误。
   ● 进行数据淘汰
   针对「进行数据淘汰」这一类策略，又可以细分为「在设置了过期时间的数据中进行淘汰」和「在所有数据范围内进行淘汰」这两类策略。
   在设置了过期时间的数据中进行淘汰：
   ○ volatile-random：随机淘汰设置了过期时间的任意键值；
   ○ volatile-ttl：优先淘汰更早过期的键值。
   ○ volatile-lru（Redis3.0 之前，默认的内存淘汰策略）：淘汰所有设置了过期时间的键值中，最久未使用的键值；
   ○ volatile-lfu（Redis 4.0 后新增的内存淘汰策略）：淘汰所有设置了过期时间的键值中，最少使用的键值；
   在所有数据范围内进行淘汰：
   ○ allkeys-random：随机淘汰任意键值;
   ○ allkeys-lru：淘汰整个键值中最久未使用的键值；
   ○ allkeys-lfu（Redis 4.0 后新增的内存淘汰策略）：淘汰整个键值中最少使用的键值。
   Redis 哈希槽的概念，数据是如何进行分片的？
   当 Redis 缓存数据量大到一台服务器无法缓存时，就需要使用 Redis 切片集群（Redis Cluster ）方案，它将数据分布在不同的服务器上，以此来降低系统对单主节点的依赖，从而提高 Redis 服务的读写性能。
   Redis Cluster 方案采用哈希槽（Hash Slot），来处理数据和节点之间的映射关系。在 Redis Cluster 方案中，一个切片集群共有 16384 个哈希槽，这些哈希槽类似于数据分区，每个键值对都会根据它的 key，被映射到一个哈希槽中，具体执行过程分为两大步：
   ● 根据键值对的 key，按照 CRC16 算法 (opens new window)计算一个 16 bit 的值。
   ● 再用 16bit 值对 16384 取模，得到 0~16383 范围内的模数，每个模数代表一个相应编号的哈希槽。
   为何redis集群用哈希槽，而不用一致性哈希？
   ● Hash slot(slot 空间)对比一致性哈希(环空间) 可以做到数据分配更均匀
   ○ 有 N 个节点,每个节点是准确的承担 1/N 的容量
   ○ 一致性哈希，它使用的是hash函数返回的值是随机的。
   ● Hash slot 更便捷的新增/删除节点
   ○ 假设已有R1、R2、R3 节点
   ○ 若新增 R4 节点，只需要从 R1、R2、R3 挪一部分 slot 到 R4 上
   ○ 若删除 R1  节点，只需将 R1 中 slot 移到 R2 和 R3 节点上, 节点之间的槽移动不会停止服务，集群是一直可用状态。
   ● 当一致性哈希增删节点时，会导致部分数据无法命中，严重的甚至会导致缓存雪崩。
   一致性hash是通过虚拟节点去避免服务节点宕机后数据转移造成的服务访问量激增、内存占用率过高、数据倾斜等问题，保证数据完整性和集群可用性的；而hash集群是使用主从节点的形式，主节点提供读写服务，从节点进行数据同步备份，当主节点出现故障后，从节点继续提供服务。
   Redis 脑裂问题
   ● 脑裂：Redis的集群脑裂是指因为网络问题导致主节点跟从节点和哨兵处于不同的网络分区，由于哨兵无法感知到主节点存在，所以将从节点提升为主节点，同时存在两个不同的主节点。
   ● 影响：如果客户端还向原来的主节点继续写入数据，那么新的主节点将无法同步这些数据，当网络问题解决之后，哨兵将原先的主节点降为从节点，此时再从新的主节点中同步数据，将会造成大量的数据丢失。
   ● 解决：Redis的配置文件中，存在两个参数。
   ○ min-slaves-to-write：表示连接到主节点的最少从节点数量，判断有多少个从服务器，达到了要求才发送信息，避免了主服务器失连后依旧写入数据；
   ○ min-slaves-max-lag：表示从节点连接到主节点的最大延迟时间，减少同步间隔时间；
   ○ 有了这两个配置就可以轻松解决脑裂问题了，首先既然原主节点是假故障，它在假故障期间是无法响应哨兵心跳的，也不能和从节点进行同步，自然也就无法和从节点进行 ACK 确认了。这样一来 min-slaves-to-write 和 min-slaves-max-lag 的组合要求就无法得到满足，原主节点就会被限制接收客户端请求，客户端也就不能在原主节点中写入新数据了。等到新主节点上线时，就只有新主节点能接收和处理客户端请求，此时，新写的数据会被直接写到新主节点中。而原主节点会被哨兵降为从节点，即使它的数据被清空了，也不会有新数据丢失。

Redis 集群
● 主从复制：主从其实就是一般包含一个主，一个或多个从，从节点从主节点复制数据，可以实现读写分离，主节点做写，从节点做读。
● 哨兵机制：哨兵模式也是一种主从，只不过增加了哨兵的功能，用于监控主节点的状态，当主节点宕机之后会进行投票重新选出主节点。
● Cluster：集群采用了多主多从，按照一定的规则进行分片，将数据分别存储，一定程度上解决了哨兵模式下单机存储有限的问题。


Redis 主从复制的实现
Kafka
https://cloud.tencent.com/developer/article/1639123
为什么需要消息系统？
● 解耦：允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。
● 扩展性：因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。
● 冗余：消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的”插入-获取-删除”范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。
● 灵活性 & 峰值处理能力：在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。
● 缓冲：有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。
● 可恢复性：系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。
● 顺序保证：在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。（Kafka 保证一个 Partition 内的消息的有序性）
● 异步处理：不需要立即处理消息，消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。
数据传输的事务定义有哪三种？
和 MQTT 的事务定义一样都是 3 种：
● 最多一次：消息不会被重复发送，最多被传输一次，但也有可能一次不传输；
● 最少一次：消息不会被漏发送，最少被传输一次，但也有可能被重复传输；
● 精确的一次（Exactly once）：不会漏传输也不会重复传输,每个消息都传输被一次而且仅仅被传输一次，这是大家所期望的；
Kafka是怎么实现Exactly Once的？
在实际情况下，我们对于某些比较重要的消息，需要保证exactly once语义，也就是保证每条消息被发送且仅被发送一次，不能重复。在0.11版本之后，Kafka引入了幂等性机制（idempotent），配合acks = -1时的at least once语义，实现了producer到broker的exactly once语义。
idempotent + at least once = exactly once
使用时，只需将enable.idempotence属性设置为true，kafka自动将acks属性设为-
kafka 消息是有序的吗？
Kafka只能保证局部有序，即只能保证一个分区里的消息有序。如果一个主题有多个分区，那么Kafka会按照key将其发送到对应的分区中，所以，对于给定的key，与其对应的record在分区内是有序的。
● 方案一：kafka topic 只设置一个partition分区 (但是会严重影响吞吐量)
● 方案二：producer将消息发送到指定同一个partition分区
kafka 维护消费状态跟踪的方法?
部分消息系统在 broker 端的维护消息被消费的记录：一个消息被分发到consumer 后 broker 就马上进行标记或者等待 customer 的通知后进行标记。这样也可以在消息在消费后立马就删除以减少空间占用。
但是这样会不会有什么问题呢？如果一条消息发送出去之后就立即被标记为消费过的，旦 consumer 处理消息时失败了（比如程序崩溃）消息就丢失了。为了解决这个问题，很多消息系统提供了另外一个个功能：当消息被发送出去之后仅仅被标记为已发送状态，当接到 consumer 已经消费成功的通知后才标记为已被消费的状态。这虽然解决了消息丢失的问题，但产生了新问题，首先如果 consumer处理消息成功了但是向 broker 发送响应时失败了，这条消息将被消费两次。第二个问题时，broker 必须维护每条消息的状态，并且每次都要先锁住消息然后更改状态然后释放锁。这样麻烦又来了，且不说要维护大量的状态数据，比如如果消息发送出去但没有收到消费成功的通知，这条消息将一直处于被锁定的状态。
Kafka 采用了不同的策略：Topic 被分成了若干分区，每个分区在同一时间只被一个 consumer 消费。这意味着每个分区被消费的消息在日志中的位置仅仅是一个简单的整数 offset。这样就很容易标记每个分区消费状态就很容易了，仅仅需要一个整数而已。这样消费状态的跟踪就很简单了。
这带来了另外一个好处：consumer 可以把 offset 调成一个较老的值，去重新消费老的消息。
kafka是怎么维护offset的吗？
● 维护offset的原因：由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以consumer需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费。
● 维护offset的方式：Kafka 0.9版本之前，consumer默认将offset保存在Zookeeper中，从0.9版本开始，consumer默认将offset保存在Kafka一个内置的topic中，该topic为**__consumer_offsets**。
● 需要掌握的关于offset的常识：消费者提交消费位移时提交的是当前消费到的最新消息的offset+1而不是offset。
kafka消息的确认机制，如何保证数据可靠性？
为保证 producer 发送的数据，能可靠的发送到指定的 topic，topic 的每个 partition 收到 producer 发送的数据后，都需要向 producer 发送 ack 回应，如果 producer 收到 ack，就会进行下一轮的发送，否则重新发送数据。消息的确认机制如下。
● acks=0（默认）：如果设置为0，生产者不会等待kafka的响应。
● acks=1：这个配置意味着kafka会把这条消息写到本地日志文件中，但是不会等待集群中其他机器的成功响应。
● acks=all：这个配置意味着leader会等待所有的follower同步完成。这个确保消息不会丢失，除非kafka集群中所有机器挂掉。这是最强的可用性保证。
多少个follower同步完成后发送ack？
● 半数以上的follower同步完成，即可发送ack。优点：延迟低，反应速度快。 缺点：选举新的leader时，若要容忍n台结点的故障需要2n+1个副本。
● 全部的follower同步完成，才可以发送ack，kafka选择了该方案。优点：选举新的leader时，容忍n台结点的故障，需要n+1副本。缺点：延迟高，反应速度慢。
kafka选择了第二种方案原因如下：
● 为了容忍n台节点的故障，第一种方案需要2n+1个副本，而第二种方案只需要n+1个副本。kafka的每个分区都有大量的数据，第一种方案会造成大量的数据冗余
● 虽然第二种方案的网络延迟会比较高，但网络延迟对kafka的影响较小
为何是2n+1？
● 节点需要超过一半才能选举一个新的leader。
● 如果只有2n个副本，每次同步到n个节点就返回，如果这n个节点都发生故障，数据便丢失了
● 如果是2n+1个副本，半数就是n+1，即每次同步到n+1个节点就返回，n个节点发生故障，至少还存在一个节点存在完整数据。
Kafka中的HW、LEO、ISR、AR分别是什么意思？
● LEO：每个副本的最后一条消息的offset
● HW：一个分区中所有副本最小的offset
● ISR：与leader保持同步的follower集合
● AR：分区的所有副本
ISR就是kafka的副本同步队列，全称是In-Sync Replicas。ISR 中包括 Leader 和 Follower。如果 Leader 进程挂掉，会在 ISR 队列中选择一个服务作为新的 Leader。有 replica.lag.max.messages（延 迟条数）和replica.lag.time.max.ms（延迟时间）两个参数决定一台服务是否可以加入 ISR 副 本队列，在 0.10 版本移除了 replica.lag.max.messages 参数，防止服务频繁的进去队列。任意一个维度超过阈值都会把 Follower 剔除出 ISR，存入 OSR（Outof-Sync Replicas） 列表，新加入的 Follower 也会先存放在 OSR 中。
Kafka 是推模式还是拉模式？
Kafka 遵循了一种大部分消息系统共同的传统的设计，producer 将消息推送到 broker，consumer 从 broker 拉取消息。
推模式 和 拉模式的区别：
推模式的实现是 consumer 与 broker 建立长连接，当有消息时 broker 会通过长连接通道将消息推送给 consumer，这样 consumer 就能实时消费到最新的消息。
优点：
● 实时性强，有消息立马推送给 consumer；
● consumer 实现简单，只需要监听 broker 的推送即可；
缺点：
● consumer 容易发生消息堆积的情况，因为每个客户端的消费能力是不同的；
● 服务端逻辑复杂，因为简单的推送会导致客户端出现堆积问题，所以服务端需要进行优化。记录给每个客户端的推送数据，然后根据每个客户端的消费能力去平衡数据推送的速度；
拉模式就是主动去拉取消息，拉模式肯定不能用传统的定时拉取，定时长及时性无法保证，定时短，在没有消息的情况下对服务端会一直请求。所以很多拉模式都是基于长轮询来实现。
优点：
● 不会造成 consumer 消息积压，消费完了再去拉取；
● 长轮询实现的拉模式实时性也能够保证。
缺点：
consumer 的逻辑实现相对复杂，简化了 broker 的逻辑。
Kafka 为什么这么快，吞吐量高？
顺序写入：Kafka会消息都持久化到硬盘中，每一个Partition其实都是一个文件 ，收到消息后Kafka会把数据追加到文件末尾，这种顺序写入的方式要比随机写入的方式快得多。
数据压缩：Kafka速度的秘诀在于，它把所有的消息都变成一个批量的文件，并且进行合理的批量压缩，减少网络IO损耗，通过mmap提高I/O速度，写入数据的时候由于单个Partion是末尾添加所以速度最优；读取数据的时候配合sendfile直接暴力输出。
零拷贝：减少了数据复制，还减少了上下文切换的消耗；
批量压缩：Kafka使用了批量压缩，将多个消息一起压缩而不是单个消息压缩；
Kafka是如何保障数据不丢失的？
broker端:
对于Kafka的Broker而言，Kafka 的复制机制和分区的多副本架构是Kafka 可靠性保证的核心。把消息写入多个副本可以使Kafka 在发生崩溃时仍能保证消息的持久性。Producer ack -1(all). 能够保证所有的副本都同步好了数据。其中一台机器挂了，并不影响数据的完整性。
消费端：
通过offset commit 来保证数据的不丢失，kafka自己记录了每次消费的offset数值，下次继续消费的时候，会接着上次的offset进行消费。  
而offset的信息在kafka0.8版本之前保存在zookeeper中，在0.8版本之后保存到topic中，即使消费者在运行过程中挂掉了，再次启动的时候会找到offset的值，找到之前消费消息的位置，接着消费，由于offset的信息写入的时候并不是每条消息消费完成后都写入的，所以这种情况有可能会造成重复消费，但是不会丢失消息。
Kafka可以保障永久不丢失数据吗？
Kafka 只对“已提交”的消息（committed message）做有限度的持久化保证。所以说，Kafka不能够完全保证数据不丢失，需要做出一些权衡。
首先，要理解什么是已提交的消息，当 Kafka 的若干个 Broker 成功地接收到一条消息并写入到日志文件后，它们会告诉生产者程序这条消息已成功提交。此时，这条消息在 Kafka 看来就正式变为已提交消息了。所以说无论是ack=all，还是ack=1,不论哪种情况，Kafka 只对已提交的消息做持久化保证这件事情是不变的。
其次，要理解有限度的持久化保证，也就是说 Kafka 不可能保证在任何情况下都做到不丢失消息。必须保证Kafka的Broker是可用的，换句话说，假如消息保存在 N 个 Kafka Broker 上，那么这个前提条件就是这 N 个 Broker 中至少有 1 个存活。只要这个条件成立，Kafka 就能保证你的这条消息永远不会丢失。
总结一下，Kafka 是能做到不丢失消息的，只不过这些消息必须是已提交的消息，而且还要满足一定的条件。
kafka 如何不消费重复数据？
● 底层根本原因：已经消费了数据，但是offset没提交。
● 重复消费最常见的情况：re-balance问题,通常会遇到消费的数据，处理很耗时，导致超过了Kafka的session timeout时间（0.10.x版本默认是30秒），那么就会re-balance重平衡，此时有一定几率offset没提交，会导致重平衡后重复消费。
● 解决办法：至少成功发送一次+去重操作（幂等性）
● 保证不丢失消息：生产者（ack=all 代表至少成功发送一次)；消费者 （offset手动提交，业务逻辑成功处理后，提交offset）
● 去重问题：消息可以使用唯一id标识
● 保证不重复消费：落表（主键或者唯一索引的方式，避免重复数据）
为什么kafka中1个partition只能被同组的一个consumer消费?
Kafka通过消费者组机制同时实现了发布/订阅模型和点对点模型。多个组的消费者消费同一个分区属于多订阅者的模式，自然没有什么问题；而在单个组内某分区只交由一个消费者处理的做法则属于点对点模式。其实这就是设计上的一种取舍，如果Kafka真的允许组内多个消费者消费同一个分区，也不是什么灾难性的事情，只是没什么意义，而且还会重复消费消息。通常情况下，我们还是希望一个组内所有消费者能够分担负载，让彼此做的事情没有交集，做一些重复性的劳动纯属浪费资源。
当Kafka消息数据出现了积压，应该怎么处理？
数据积压主要可以从两个角度去分析：
● 如果是 Kafka 消费能力不足，则可以考虑增加 Topic 的分区数，并且同时提升消费 组的消费者数量，消费者数=分区数。（两者缺一不可）
● 如果是下游的数据处理不及时：提高每批次拉取的数量。如果是因为批次拉取数据过少（拉取 数据/处理时间<生产速度），也会使处理的数据小于生产的数据，造成数据积压。
Rebalance机制是什么？
Rebalance 的触发条件有3个:
● 组成员个数发生变化。例如有新的 consumer 实例加入该消费组或者离开组。
● 订阅的 Topic 个数发生变化。
● 订阅 Topic 的分区数发生变化。
Rebalance 发生时，Group 下所有 consumer 实例都会协调在一起共同参与，kafka 能够保证尽量达到最公平的分配。但是 Rebalance 过程对 consumer group 会造成比较严重的影响。在 Rebalance 的过程中 consumer group 下的所有消费者实例都会停止工作，等待 Rebalance 过程完成。
Kafka消费者分区分配策略有哪些？
● RoundRobin 是针对所有topic分区。它是采用轮询分区策略，是把所有的partition和所有的consumer列举出来，然后按照hashcode进行排序，最后再通过轮询算法来分配partition给每个消费者。
● Range 策略是kafka默认的消费者分区分配策略，它是针对topic维度的，首先对同一个topic里面的分区按照序号进行排序，并对消费者按照字母顺序进行排序。容易产生数据倾斜！
● Sticky 策略，粘性分区定义:可以理解为分配的结果带有“粘性的”。即在执行一次新的分配之前，考虑上一次分配的结果。尽量少的调整分配的变动，可以节省大量的开销。首先会尽量均衡的分配分区到消费者上面，在出现同一消费组内消费者出现问题的时候，会尽量保持原来的分配的分区不变。

数据如何保存到硬盘？
topic 中的多个 partition 以文件夹的形式保存到 broker，每个分区序号从 0 递增，且消息有序。
Partition 文件下有多个 segment（xxx.index，xxx.log）
segment 文件里的 大小和配置文件大小一致可以根据要求修改，默认为 1g。
如果大小大于 1g 时，会滚动一个新的 segment 并且以上一个 segment 最后一条消息的偏移量命名。
你了解Kafka的日志目录结构吗？
● 1每个 Topic 都可以分为一个或多个 Partition，Topic其实是比较抽象的概念，但是 Partition是比较具体的东西；
●  其实Partition 在服务器上的表现形式就是一个一个的文件夹，由于生产者生产的消息会不断追加到log文件末尾，为防止log文件过大导致数据定位效率低下，Kafka采取了分片和索引机制，将每个partition分为多个segment；
●  每组 Segment 文件又包含 .index 文件、.log 文件、.timeindex 文件（早期版本中没有）三个文件。.log和.index文件位于一个文件夹下，该文件夹的命名规则为：topic名称+分区序号。例如，csdn这个topic有2个分区，则其对应的文件夹为csdn-0,csdn-1；
●  log 文件就是实际存储 Message 的地方，而 index 和 timeindex 文件为索引文件，用于检索消息

